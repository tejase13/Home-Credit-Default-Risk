{
  "cells": [
    {
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true
      },
      "cell_type": "code",
      "source": "# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import KFold\nimport gc\nimport matplotlib.pyplot as plt\nfrom lightgbm import LGBMClassifier",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "4c5da194d56307188d8af2a8ae92229f1367f729"
      },
      "cell_type": "markdown",
      "source": "### Processing Application data"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "2645a420911d6b99dc5c5682722121f28b843fd7"
      },
      "cell_type": "code",
      "source": "print(pd.read_csv(\"../input/application_train.csv\").shape[0] + pd.read_csv(\"../input/application_test.csv\").shape[0])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "scrolled": true,
        "_uuid": "78f333b412e0f18cb58fb860b9b4c255d18d16bc"
      },
      "cell_type": "code",
      "source": "application_train = pd.read_csv(\"../input/application_train.csv\")\napplication_test = pd.read_csv(\"../input/application_test.csv\")\n\napplication_train[\"source\"] = \"train\"\napplication_test[\"source\"] = \"test\"\n\n''' 1. Append train and test data'''\napplication = pd.concat([application_train, application_test], axis=0, ignore_index=True)\n\n'''2. Find columns with a lot of missing data and remove uncorrelated variables'''\nmissing_cols_prcnt = application_train.isnull().sum()/application_train.shape[0] * 100\nhigh_missing_values = missing_cols_prcnt[missing_cols_prcnt > 50]\nhigh_missing_values_index = high_missing_values.index.tolist()\n\ncorrelations = application_train.corr()['TARGET'].sort_values()\ncorr_missing_cols = correlations.reindex(high_missing_values_index).sort_values()\nmissing_cols_to_be_dropped = corr_missing_cols.index.difference(corr_missing_cols[(corr_missing_cols > 0.02) | (corr_missing_cols < -0.02)].index).tolist()\nlen(missing_cols_to_be_dropped)\n\napplication = application.drop(missing_cols_to_be_dropped, axis=1)\n\n'''3. Separate numerical and categorical data'''\n#Make sure 'TARGET' and 'Source' are excluded\ncategorical_vars = [f for f in application.columns if f!='TARGET' and f!='source' and application[f].dtype==\"object\"]\n#Check once more that all numerical columns are indeed continuous and not discrete\nnumerical_vars = [f for f in application.columns if f!='TARGET' and f!='source' and application[f].dtype!=\"object\"]\n\n'''4. DAYS_EMPLOYED discrepancy'''\napplication[\"DAYS_EMPLOYED_ANOMALY\"] = 0\napplication[\"DAYS_EMPLOYED_ANOMALY\"].loc[application[application[\"DAYS_EMPLOYED\"] > 0][\"DAYS_EMPLOYED\"].index] = 1\napplication[\"DAYS_EMPLOYED\"].loc[application[application[\"DAYS_EMPLOYED\"] > 0][\"DAYS_EMPLOYED\"].index] = 0\napplication[[\"DAYS_EMPLOYED\", \"DAYS_EMPLOYED_ANOMALY\"]].head()\n\n'''5. XNA represents null in categiorical cols'''\nxna_cols = []\nfor c in categorical_vars:\n    if application[c][application[c]=='XNA'].shape[0] > 0:\n        xna_cols.append(c)\n        \nfor c in xna_cols:\n    application[c] = application[c].replace('XNA', np.nan)\n    \n'''6. Amt credit missing values imputation'''\nmissing_goods_price_indices = application[application[\"AMT_GOODS_PRICE\"].isnull()].index\nnorm_factor = np.mean(application[\"AMT_GOODS_PRICE\"])/np.mean(application[\"AMT_CREDIT\"])\n#Replacing the missing values of AMT_GOODS_PRICE\napplication.loc[missing_goods_price_indices, \"AMT_GOODS_PRICE\"] = application.loc[missing_goods_price_indices, \"AMT_CREDIT\"] * norm_factor\n\n'''7. For other numerical columns impute missing values by median(including ext_source_3)'''\nnumeric_cols_with_missing_vals = application[numerical_vars].isnull().sum()[application[numerical_vars].isnull().sum() > 0].index.tolist()\ncategorical_cols_with_missing_vals = application[categorical_vars].isnull().sum()[application[categorical_vars].isnull().sum() > 0].index.tolist()\n\n#np.median(application[\"AMT_ANNUITY\"].dropna())\nfor f in numeric_cols_with_missing_vals:\n    application[f] = application[f].replace(np.nan, np.median(application[f].dropna()))\n    \n# application[\"CODE_GENDER\"].value_counts().idxmax()\nfor f in categorical_cols_with_missing_vals:\n    max_count = application[f].value_counts().idxmax()\n    application[f] = application[f].fillna(max_count)\n    \n'''8. Adding domain specific columns as mentioned in other kernels'''\napplication['CREDIT_INCOME_PERCENT'] = application['AMT_CREDIT'] / application['AMT_INCOME_TOTAL']\napplication['ANNUITY_INCOME_PERCENT'] = application['AMT_ANNUITY'] / application['AMT_INCOME_TOTAL']\napplication['CREDIT_TERM'] = application['AMT_ANNUITY'] / application['AMT_CREDIT']\napplication['DAYS_EMPLOYED_PERCENT'] = application['DAYS_EMPLOYED'] / application['DAYS_BIRTH']\n\n'''9. One hot encoding of categorical variables'''\ndummies = pd.get_dummies(application[categorical_vars])\napplication = pd.concat([application, dummies], axis=1)\napplication = application.drop(categorical_vars, axis=1)\napplication.head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "6a3e61706cc22a3c265258306945ea924cbe31e2"
      },
      "cell_type": "code",
      "source": "application.shape[0]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "346bc7ea3f0fbcedea5d97003d7fe312522c2885"
      },
      "cell_type": "markdown",
      "source": "### Processing Bureau and Bureau_Balance datasets"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "abdd388efacff16b7546ee28f3ce63f4645b108e",
        "scrolled": true
      },
      "cell_type": "code",
      "source": "'''1. Bureau_balance'''\nbureau_balance = pd.read_csv(\"../input/bureau_balance.csv\")\n\n#Get dummy variables for STATUS\ndummies = pd.get_dummies(bureau_balance['STATUS'])\ndummies.columns = ['STATUS_'+val for val in dummies.columns.values]\nbureau_balance = pd.concat([bureau_balance, dummies], axis=1)\nbureau_balance = bureau_balance.drop('STATUS', axis=1)\n\n#Aggregate the dataframe at SK_ID_BUREAU level\ngroupby_dict = {'MONTHS_BALANCE':[np.size]}\nfor col in dummies.columns:\n    groupby_dict[col] = sum\nbureau_balance_agg = bureau_balance.groupby('SK_ID_BUREAU').agg(groupby_dict)\nbureau_balance_agg.columns = [col+\"_\"+func for col, func in bureau_balance_agg.columns.values]\nbureau_balance_agg = bureau_balance_agg.reset_index()\n\n#Calling garbage collector\ngc.enable()\ndel bureau_balance, dummies\ngc.disable()\n\n# bureau_balance_agg.head()\n'''Bureau dataset'''\nbureau = pd.read_csv(\"../input/bureau.csv\")\nbureau.head()\nbureau = bureau.merge(right=bureau_balance_agg, right_on='SK_ID_BUREAU', left_on='SK_ID_BUREAU')\n\n#Replacing NA's in DAYS_CREDIT_ENDDATE\nactive_cred = float(bureau[bureau[\"CREDIT_ACTIVE\"] == 'Active'][[\"DAYS_CREDIT_ENDDATE\"]].median())\nclosed_cred = float(bureau[bureau[\"CREDIT_ACTIVE\"] == 'Closed'][[\"DAYS_CREDIT_ENDDATE\"]].median())\nbureau.loc[bureau[bureau[\"CREDIT_ACTIVE\"] == 'Active'][bureau[\"DAYS_CREDIT_ENDDATE\"].isnull()].index, \"DAYS_CREDIT_ENDDATE\"] = active_cred\nbureau.loc[bureau[bureau[\"CREDIT_ACTIVE\"] == 'Closed'][bureau[\"DAYS_CREDIT_ENDDATE\"].isnull()].index, \"DAYS_CREDIT_ENDDATE\"] = closed_cred\n\n#For days_enddate_fact, we will be taking average of only the closed credits\n#So will be aggregating only at closed level and then merging to the aggregated dataset at SK_ID_CURR\n#Missing value imputation will be done only for closed credits\nenddate_closed_median = float(bureau[bureau[\"CREDIT_ACTIVE\"] == 'Closed'][[\"DAYS_ENDDATE_FACT\"]].median())\nbureau.loc[bureau[bureau[\"CREDIT_ACTIVE\"] == 'Closed'][bureau[\"DAYS_ENDDATE_FACT\"].isnull()].index, \"DAYS_ENDDATE_FACT\"] = enddate_closed_median\n\n#Creating dummy variables\nbureau_dummies = pd.get_dummies(bureau[[\"CREDIT_ACTIVE\", \"CREDIT_CURRENCY\", \"CREDIT_TYPE\"]])\nbureau = pd.concat([bureau, bureau_dummies], axis=1)\nbureau = bureau.drop([\"CREDIT_ACTIVE\", \"CREDIT_CURRENCY\", \"CREDIT_TYPE\"], axis=1)\n\n#Aggregating Bureau data\nbureau_cols = bureau.columns.tolist()\nnumeric_cols_mean = ['DAYS_CREDIT', 'CREDIT_DAY_OVERDUE',\n       'DAYS_CREDIT_ENDDATE', 'DAYS_ENDDATE_FACT', 'AMT_CREDIT_MAX_OVERDUE', 'AMT_CREDIT_SUM_OVERDUE', 'DAYS_CREDIT_UPDATE',\n       'AMT_ANNUITY',]\ncredit_sum_cols = ['AMT_CREDIT_SUM', 'AMT_CREDIT_SUM_DEBT',\n       'AMT_CREDIT_SUM_LIMIT',]\nstatus_cols = [col for col in bureau_cols if col.find('STATUS')!=-1]\ncredit_cols = [col for col in bureau_cols if col.find('CREDIT_TYPE')!=-1 or col.find('CREDIT_ACTIVE')!=-1 or col.find('CREDIT_CURRENCY')!=-1]\nagg_dict = {}\nfor col in status_cols:\n    agg_dict[col] = np.mean\nfor col in credit_cols:\n    agg_dict[col] = sum \nfor col in numeric_cols_mean:\n    agg_dict[col] = np.mean\nfor col in credit_sum_cols:\n    agg_dict[col] = [np.mean, sum]\nbureau_agg = bureau.groupby('SK_ID_CURR').agg(agg_dict)\nbureau_agg.columns = [col+\"_\"+func for col, func in bureau_agg.columns.values]\nbureau_agg = bureau_agg.reset_index()\n\ngc.enable()\ndel bureau\ngc.disable()\n\nbureau_agg.head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "dd17e7f059cfdf87782988fb551deaf1dd7dfcf5",
        "scrolled": true
      },
      "cell_type": "code",
      "source": "application = application.merge(bureau_agg, how='left', left_on=\"SK_ID_CURR\", right_on=\"SK_ID_CURR\")\ngc.enable()\ndel bureau_agg\ngc.disable()\napplication.head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "0d99ce7e4c2834c0d96a19cf6d3c71a88c9eb8d4"
      },
      "cell_type": "markdown",
      "source": "### Processing Previous Applications"
    },
    {
      "metadata": {
        "trusted": true,
        "scrolled": true,
        "_uuid": "e7a09077cb1469b28bbf8cd77a2a215430c9618c"
      },
      "cell_type": "code",
      "source": "prev_application = pd.read_csv(\"../input/previous_application.csv\")\n# Days 365.243 values -> nan\nprev_application['DAYS_FIRST_DRAWING'].replace(365243, np.nan, inplace= True)\nprev_application['DAYS_FIRST_DUE'].replace(365243, np.nan, inplace= True)\nprev_application['DAYS_LAST_DUE_1ST_VERSION'].replace(365243, np.nan, inplace= True)\nprev_application['DAYS_LAST_DUE'].replace(365243, np.nan, inplace= True)\nprev_application['DAYS_TERMINATION'].replace(365243, np.nan, inplace= True)\n\n#Missing value treatment\nmissing_values_prcnt = prev_application.isnull().sum()/prev_application.shape[0] * 100\nhigh_prop_missing_cols = list(missing_values_prcnt[missing_values_prcnt > 50].index)\n#Dropping columns with high missing values proportion\nprev_application = prev_application.drop(high_prop_missing_cols, axis=1) \nmissing_values_cols_sample = list(prev_application.isnull().sum()[prev_application.isnull().sum() > 0].index)\n\n#Missing value treatment; Numerical features will be treated by inserting 0's and Categorical features will be treated by inputting the mode\n#Actually a better technique for categorical features will be mode based on grouping SK_ID_CURR\n#Missing value imputation\nfor col in missing_values_cols_sample:\n    if prev_application[col].dtype != \"object\":\n        prev_application[col] = prev_application[col].fillna(0)\n    elif prev_application[col].dtype == \"object\":\n        prev_application[col] = prev_application[col].mode()\n\n\n#Hour process column is actually categorical. Let's convert it into one\nprev_application['HOUR_APPR_PROCESS_START'] = prev_application['HOUR_APPR_PROCESS_START'].map(str)\ncategorical_features = [f for f in prev_application.columns if prev_application[f].dtype==\"object\"]\nprev_application_with_dummy = pd.get_dummies(prev_application, columns=categorical_features)\ndummy_columns = [f for f in prev_application_with_dummy.columns if f not in prev_application.columns]#Getting the list of all newly created dummy columns\n\n\n#Rolling up values at sk_id_curr level. All the columns will use mean as aggregation function \nprev_application_with_dummy = prev_application_with_dummy.drop('SK_ID_PREV', axis=1)\nprev_agg = prev_application_with_dummy.groupby('SK_ID_CURR').agg('mean')\nprev_agg = prev_agg.reset_index()\n\ngc.enable()\ndel prev_application, prev_application_with_dummy\ngc.disable()\n\nprev_agg.head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "9dcab2f673fea125cc5d32ae158ce34347b5685d",
        "scrolled": false
      },
      "cell_type": "code",
      "source": "application = application.merge(prev_agg, how='left', left_on='SK_ID_CURR', right_on='SK_ID_CURR')\ngc.enable()\ndel prev_agg\ngc.disable()\napplication.head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "ab97d5d3a8e15a1e8b693821766d9c5e5a7503f3"
      },
      "cell_type": "markdown",
      "source": "## Processing POS CASH BALANCE"
    },
    {
      "metadata": {
        "trusted": true,
        "scrolled": true,
        "_uuid": "952812072047251ec68d8de3c32546d82b93e412"
      },
      "cell_type": "code",
      "source": "#Aggregating at SK_ID_PREV level\npos_cash = pd.read_csv(\"../input/POS_CASH_balance.csv\")\nsk_id_df = pos_cash[['SK_ID_PREV', 'SK_ID_CURR']].drop_duplicates()\n\npos_cash = pos_cash.drop(['MONTHS_BALANCE', 'CNT_INSTALMENT_FUTURE'], axis=1)\n\npos_cash = pd.get_dummies(pos_cash)\npos_cash = pos_cash.drop(['NAME_CONTRACT_STATUS_Active', 'NAME_CONTRACT_STATUS_Signed'], axis=1)\n\nagg_dict = {\n    'CNT_INSTALMENT': np.mean, \n    'SK_DPD': [sum, np.mean], \n    'SK_DPD_DEF': [sum, np.mean], \n}\nname_contract_cols = [col for col in pos_cash.columns if col.find('NAME_CONTRACT_STATUS_')!= -1]\nfor col in name_contract_cols:\n    agg_dict[col] = sum\npos_cash_agg = pos_cash.groupby('SK_ID_PREV').agg(agg_dict)\n\ngc.enable()\ndel pos_cash\ngc.disable()\n\npos_cash_agg.columns = [col+\"_\"+func for col, func in pos_cash_agg.columns] \npos_cash_agg = pos_cash_agg.reset_index()\npos_cash_final = pos_cash_agg.merge(sk_id_df, how='left', left_on='SK_ID_PREV', right_on='SK_ID_PREV')\n\ngc.enable()\ndel pos_cash_agg, sk_id_df\ngc.disable()\n\npos_cash_final.head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "scrolled": true,
        "_uuid": "d9162540bd2445551d0f6f693ac00eef9abe11ce"
      },
      "cell_type": "code",
      "source": "#Aggregating at SK_ID_CURR level\nmean_cols = [col for col in pos_cash_final.columns if col.find('mean')!= -1]\nsum_cols = [col for col in pos_cash_final.columns if col.find('sum')!= -1]\nagg_dict = {}\nfor col in mean_cols:\n    agg_dict[col] = np.mean\nfor col in sum_cols:\n    agg_dict[col] = sum\npos_cash_curr_lvl = pos_cash_final.groupby('SK_ID_CURR').agg(agg_dict)\npos_cash_curr_lvl = pos_cash_curr_lvl.reset_index()\ngc.enable()\ndel pos_cash_final\ngc.disable()\npos_cash_curr_lvl.head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "scrolled": true,
        "_uuid": "0b532592d1185b3fa9d11d987b3e8b305719fa35"
      },
      "cell_type": "code",
      "source": "application = application.merge(pos_cash_curr_lvl, how='left', left_on='SK_ID_CURR', right_on='SK_ID_CURR')\ngc.enable()\ndel pos_cash_curr_lvl\ngc.disable()\napplication.head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "013fbf6c7134a9514e783476e57d229f827630ab"
      },
      "cell_type": "markdown",
      "source": "## Processing Installments Data"
    },
    {
      "metadata": {
        "trusted": true,
        "scrolled": true,
        "_uuid": "6a39e67d19516c641f32636a8dfefff12ba090c6"
      },
      "cell_type": "code",
      "source": "#Aggregating at SK_ID_PREV level\ninstallments = pd.read_csv(\"../input/installments_payments.csv\")\n#if days_delay is a positive number then that's a bad sign\ninstallments['DAYS_DELAY'] = installments['DAYS_ENTRY_PAYMENT'] - installments['DAYS_INSTALMENT']\n#if positive value then it's bad\ninstallments['AMT_DEBT'] = installments['AMT_INSTALMENT'] - installments['AMT_PAYMENT']\nagg_dict = {\n    'NUM_INSTALMENT_VERSION': [np.size], \n    'DAYS_DELAY': [sum, np.mean, max], \n    'AMT_DEBT': [ max, sum, np.mean]\n}\ninstallments_agg = installments.groupby('SK_ID_PREV').agg(agg_dict)\ninstallments_agg.columns = [col+'_'+func for col, func in installments_agg.columns]\ninstallments_agg = installments_agg.reset_index()\nsk_id_df = installments[['SK_ID_PREV', 'SK_ID_CURR']].drop_duplicates()\ninstallments_agg = installments_agg.merge(sk_id_df, how='left', left_on='SK_ID_PREV', right_on='SK_ID_PREV')\n\n#Aggregating at SK_ID_CURR level\nmean_cols = [col for col in installments_agg.columns if col.find('mean')!= -1]\nsum_cols = [col for col in installments_agg.columns if col.find('sum')!= -1]\nmax_cols = [col for col in installments_agg.columns if col.find('max')!= -1]\nagg_dict = {}\nfor col in mean_cols:\n    agg_dict[col] = np.mean\nfor col in sum_cols:\n    agg_dict[col] = sum\nfor col in max_cols:\n    agg_dict[col] = max\nagg_dict['NUM_INSTALMENT_VERSION_size'] = np.mean\ninstallments_sk_curr_lvl = installments_agg.groupby('SK_ID_CURR').agg(agg_dict).reset_index()\ngc.enable()\ndel installments, installments_agg, sk_id_df\ngc.disable()\ninstallments_sk_curr_lvl.head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "scrolled": true,
        "_uuid": "f441d43f802c796ac83ca7587671734b537e2f9f"
      },
      "cell_type": "code",
      "source": "application = application.merge(installments_sk_curr_lvl, how='left', left_on='SK_ID_CURR', right_on='SK_ID_CURR')\ngc.enable()\ndel installments_sk_curr_lvl\ngc.disable()\napplication.head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "cbbb09a600e88a2c2cff7b4a635a1168283a3153"
      },
      "cell_type": "markdown",
      "source": "### Processing Credit Card Balance dataset"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "0b1095d2542ccc12b6b1643ff1346ab08d5ebe7d"
      },
      "cell_type": "code",
      "source": "cc = pd.read_csv('../input/credit_card_balance.csv')\ncc = pd.get_dummies(cc)\n# General aggregations\ncc.drop(['SK_ID_PREV'], axis= 1, inplace = True)\ncc_agg = cc.groupby('SK_ID_CURR').agg(['min', 'max', 'mean', 'sum', 'var'])\ncc_agg.columns = pd.Index(['CC_' + e[0] + \"_\" + e[1].upper() for e in cc_agg.columns.tolist()])\n# Count credit card lines\ncc_agg['CC_COUNT'] = cc.groupby('SK_ID_CURR').size()\ncc_agg = cc_agg.reset_index()\ndel cc\ngc.collect()\ncc_agg.head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "309e22748ebf48349a46a81d6ecb6e49bd6d83bd"
      },
      "cell_type": "code",
      "source": "application = application.merge(cc_agg, how='left', left_on='SK_ID_CURR', right_on='SK_ID_CURR')\ngc.enable()\ndel cc_agg\ngc.disable()\napplication.head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "c92e7fae773748574134d4d5bb12040a499f1dd7"
      },
      "cell_type": "markdown",
      "source": "16) LGBM K FOLD"
    },
    {
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "_uuid": "277a06a9060bfd8f01a45eecd31abd94a6045402"
      },
      "cell_type": "code",
      "source": "X_train = application[application[\"source\"] == 'train'].drop([\"source\", \"TARGET\"], axis=1)\nY_train = application[application[\"source\"] == 'train'][\"TARGET\"]\nX_test = application[application[\"source\"] == 'test'].drop([\"source\", \"TARGET\"], axis=1)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "45660acf5f1534b7960f63b9c57aa85d0fb56888"
      },
      "cell_type": "code",
      "source": "X_test.shape",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "5a28b8c5483b1fbf79a40a047911b693342ba28e",
        "scrolled": true
      },
      "cell_type": "code",
      "source": "k_fold = KFold(n_splits = 5, shuffle = True, random_state = 50)\nx_train = np.array(X_train.values)\ny_train = np.array(Y_train.values)\nx_test = np.array(X_test.values)\ntest_predictions = np.zeros(x_test.shape[0])\ntrain_auc = []\nvalid_auc = []\nfor train_indices, valid_indices in k_fold.split(x_train):\n    train_data, train_target = x_train[train_indices], y_train[train_indices]\n    valid_data, valid_target = x_train[valid_indices], y_train[valid_indices]\n    clf = LGBMClassifier(\n        n_estimators=1000,\n        learning_rate=0.03,\n        num_leaves=30,\n        colsample_bytree=.8,\n        subsample=.9,\n        max_depth=7,\n        reg_alpha=.1,\n        reg_lambda=.1,\n        min_split_gain=.01,\n        min_child_weight=2,\n        silent=-1,\n        verbose=-1,\n    )\n        \n    clf.fit(train_data, train_target, \n        eval_set= [(train_data, train_target), (valid_data, valid_target)], \n        eval_names = ['train', 'valid'],\n        eval_metric='auc', verbose=100, early_stopping_rounds=100  #30\n   )\n    \n    #best_iteration = clf.best_iteration_\n    \n    valid_score = clf.best_score_['valid']['auc']\n    train_score = clf.best_score_['train']['auc']\n    valid_auc.append(valid_score)\n    train_auc.append(train_score)\n    \n    test_predictions += clf.predict_proba(x_test, num_iteration=clf.best_iteration_)[:,1]/k_fold.n_splits\n    \n    gc.enable()\n    del clf, train_data, valid_data\n    gc.disable()\n        ",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "239a680955b43ca771eaed9d5bf9f3ba3f28bf2c"
      },
      "cell_type": "code",
      "source": "kfold_auc = pd.DataFrame({'train_auc': train_auc, \n                          'valid_auc': valid_auc})\nkfold_auc",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "dfc6b7eefa59216d2072d9b47bc7a242b759b7e7"
      },
      "cell_type": "code",
      "source": "test_output = pd.DataFrame({'SK_ID_CURR':X_test.SK_ID_CURR.values, 'TARGET':test_predictions})\ntest_output.head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "0a1d60b70a4f49e81f1ff02deb8f2489039b6631"
      },
      "cell_type": "code",
      "source": "test_output.shape",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "_uuid": "0d4669bcc97077e7b6853aa8d873f789a0515f40"
      },
      "cell_type": "code",
      "source": "test_output.to_csv('light_gbm_v7.csv', index = False)",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}